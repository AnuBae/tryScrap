{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from urllib.request import urlopen, Request\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import pandas as pd\r\n",
    "import mysql.connector\r\n",
    "\r\n",
    "# konektor ke database\r\n",
    "db = mysql.connector.connect(\r\n",
    "        host='localhost',\r\n",
    "        user='root',\r\n",
    "        password='',\r\n",
    "        database='UnNgGrape'\r\n",
    "        )\r\n",
    "cursor = db.cursor()\r\n",
    "\r\n",
    "# membaca data scrape sebagai patokan scraping\r\n",
    "cursor.execute(\"SELECT * FROM scrape\")\r\n",
    "raw_scrape = cursor.fetchall()\r\n",
    "for row in raw_scrape:\r\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------------------\")\r\n",
    "    print(\"Scrape for \"+row[0])\r\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------------------\")\r\n",
    "\r\n",
    "    # memberi nama objek untuk data yg diambil sebagai untuk mempermudah\r\n",
    "    main_link1 = row[1]\r\n",
    "    main_link2 = row[2]\r\n",
    "    tag_main = row[3]\r\n",
    "    tag_main_part = row[4]\r\n",
    "    tag_lowongan = row[5]\r\n",
    "    tag_lowongan_part = row[6]\r\n",
    "    tag_perusahaan = row[7]\r\n",
    "    tag_perusahaan_part = row[8]\r\n",
    "    tag_lokasi = row[9]\r\n",
    "    tag_lokasi_part = row[10]\r\n",
    "    raw_link = row[11]\r\n",
    "    tag_deskripsi = row[12]\r\n",
    "    tag_deskripsi_part = row[13]\r\n",
    "\r\n",
    "    # scrape berdasarkan data keyword\r\n",
    "    cursor.execute(\"SELECT * FROM keyword\")\r\n",
    "    raw_keyword = cursor.fetchall()\r\n",
    "    for row2 in raw_keyword:\r\n",
    "        # penyiapan scraping\r\n",
    "        keyword = row2[0]\r\n",
    "        main_link = main_link1 + keyword + main_link2\r\n",
    "        print(\"_____________________________________________________________________________________________________\")\r\n",
    "        print(\"From keyword: \"+keyword)\r\n",
    "        print(\"_____________________________________________________________________________________________________\")\r\n",
    "\r\n",
    "        # menyiapkan scrape berdasarkan link yang sudah disiapkan\r\n",
    "        r = Request(main_link, headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'})\r\n",
    "        response = urlopen(r).read()\r\n",
    "        print(type(response))\r\n",
    "        soup = BeautifulSoup(response, \"lxml\")\r\n",
    "        print(type(soup))\r\n",
    "\r\n",
    "        # memulai scrape\r\n",
    "        jobList = soup.find_all(tag_main, tag_main_part)\r\n",
    "        for p in jobList:\r\n",
    "            lowongan = p.find(tag_lowongan, tag_lowongan_part).get_text()\r\n",
    "            # menggunakan try except karena ada beberapa perusahaan yang dirahasiakan\r\n",
    "            try:\r\n",
    "                perusahaan = p.find(tag_perusahaan, tag_perusahaan_part).get_text()\r\n",
    "            except:\r\n",
    "                perusahaan = \"Perusahaan Dirahasiakan\"\r\n",
    "            lokasi = p.find(tag_lokasi, tag_lokasi_part).get_text()\r\n",
    "            link = raw_link + p.find('a').get(\"href\")\r\n",
    "\r\n",
    "            cursor.execute(\"SELECT * FROM lowongan WHERE link_lowongan = '\" + link + \"'\")\r\n",
    "            data = cursor.fetchall()\r\n",
    "            # if else untuk pengecekan input data\r\n",
    "            \r\n",
    "            if data:\r\n",
    "                print(\"Data exists\")\r\n",
    "            else:\r\n",
    "                try:\r\n",
    "                    #   scrape detail sesuai link\r\n",
    "                    r2 = Request(link, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'})\r\n",
    "                    response2 = urlopen(r2).read()\r\n",
    "                    soup2 = BeautifulSoup(response2, \"lxml\")\r\n",
    "                    \r\n",
    "                    raw_deskripsi = soup2.find(tag_deskripsi, tag_deskripsi_part)\r\n",
    "                    # increment deskripsi\r\n",
    "                    deskripsi = \"\"\r\n",
    "\r\n",
    "                    if raw_deskripsi is None:\r\n",
    "                        print(\"raw_deskripsi Nonetype. link: \"+ link)\r\n",
    "                    else:\r\n",
    "                        for string in raw_deskripsi.strings:\r\n",
    "                            new_string  = deskripsi + \" \" + string\r\n",
    "                            deskripsi = new_string\r\n",
    "                except:\r\n",
    "                    deskripsi = \"\"\r\n",
    "        \r\n",
    "                cursor.execute(\r\n",
    "                    \"INSERT INTO lowongan(title_lowongan, nama_perusahaan, lokasi_perusahaan, deskripsi_lowongan, link_lowongan)\"\r\n",
    "                    \"VALUES ('\"+ lowongan.replace(\"'\", \"\").replace('\"', '') +\"', '\"+ perusahaan.replace(\"'\", \"\").replace('\"', '') +\"', '\"+ lokasi.replace(\"'\", \"\").replace('\"', '') +\"', '\"+ deskripsi.replace(\"'\", \"\").replace('\"', '').replace(\"Informasi Penting Pastikan perusahaan yang kamu lamar resmi dengan memeriksa website dan lowongan kerja mereka. Read Less \", \"\") +\"', '\"+ link +\"')\"\r\n",
    "                )\r\n",
    "                db.commit()\r\n",
    "                print(\"Data has been added\")\r\n",
    "\r\n",
    "print(\"________________________________________DONE________________________________________\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Scrape for glints\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "_____________________________________________________________________________________________________\n",
      "From keyword: teknik\n",
      "_____________________________________________________________________________________________________\n",
      "<class 'bytes'>\n",
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'cursor2' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7efae4f84ab4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     92\u001b[0m                     \u001b[0mdeskripsi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 cursor2.execute(\n\u001b[0m\u001b[0;32m     95\u001b[0m                     \u001b[1;34m\"INSERT INTO lowongan(title_lowongan, nama_perusahaan, lokasi_perusahaan, deskripsi_lowongan, link_lowongan)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                     \u001b[1;34m\"VALUES ('\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mlowongan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\"', '\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mperusahaan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\"', '\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mlokasi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\"', '\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mdeskripsi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Informasi Penting Pastikan perusahaan yang kamu lamar resmi dengan memeriksa website dan lowongan kerja mereka. Read Less \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\"', '\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\"')\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cursor2' is not defined"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}