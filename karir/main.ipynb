{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd07b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d",
   "display_name": "Python 3.8.5 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<urllib.request.Request object at 0x000001D9CAB422E0>\n",
      "<class 'bytes'>\n",
      "<class 'bs4.BeautifulSoup'>\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "<urllib.request.Request object at 0x000001D9CAC00A90>\n",
      "<class 'bytes'>\n",
      "<class 'bs4.BeautifulSoup'>\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "<urllib.request.Request object at 0x000001D9CAC00BB0>\n",
      "<class 'bytes'>\n",
      "<class 'bs4.BeautifulSoup'>\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "<urllib.request.Request object at 0x000001D9CACBFD90>\n",
      "<class 'bytes'>\n",
      "<class 'bs4.BeautifulSoup'>\n",
      "Data exists\n",
      "<urllib.request.Request object at 0x000001D9CADC30D0>\n",
      "<class 'bytes'>\n",
      "<class 'bs4.BeautifulSoup'>\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "<urllib.request.Request object at 0x000001D9CAE10340>\n",
      "<class 'bytes'>\n",
      "<class 'bs4.BeautifulSoup'>\n",
      "Data exists\n",
      "Data exists\n",
      "<urllib.request.Request object at 0x000001D9CAE82670>\n",
      "<class 'bytes'>\n",
      "<class 'bs4.BeautifulSoup'>\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n",
      "Data exists\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# konektor ke database\n",
    "db = mysql.connector.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='',\n",
    "        database='UnNgGrape'\n",
    "        )\n",
    "cursor = db.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM keyword\")\n",
    "raw_keyword = cursor.fetchall()\n",
    "for row in raw_keyword:\n",
    "    # penyiapan scraping\n",
    "    keyword = row[0]\n",
    "    r = Request(\"https://www.karir.com/search?q=\" + keyword + \"&sort_order=urgent_job&job_function_ids=&industry_ids=&degree_ids=&major_ids=&location_ids=&location_id=&location=&salary_lower=0&salary_upper=100000000&page=0&grid=list\", headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'})\n",
    "    print(r)\n",
    "    response = urlopen(r).read()\n",
    "    print(type(response))\n",
    "\n",
    "    soup = BeautifulSoup(response, \"lxml\")\n",
    "    print(type(soup))\n",
    "\n",
    "    # inisialisasi array untuk menampilkan hasil\n",
    "    title_lowongan = []\n",
    "    nama_perusahaan = []\n",
    "    lokasi_perusahaan = []\n",
    "    link_lowongan = []\n",
    "    deskripsi_lowongan = []\n",
    "\n",
    "    jobList = soup.find_all(\"div\",\"row opportunity-box\")\n",
    "    i = 1\n",
    "    for p in jobList:\n",
    "        lowongan = p.find('h4', \"tdd-function-name\").get_text()\n",
    "        # menggunakan try except karena ada beberapa perusahaan yang dirahasiakan\n",
    "        try:\n",
    "            perusahaan = p.find('div', \"tdd-company-name\").get_text()\n",
    "        except:\n",
    "            perusahaan = \"Perusahaan Dirahasiakan\"\n",
    "        lokasi = p.find('span', \"tdd-location\").get_text()\n",
    "        link = \"https://m.karir.com\"+p.find('a').get('href')\n",
    "\n",
    "        cursor.execute(\"SELECT * FROM karir WHERE link_lowongan = '\" + link + \"'\")\n",
    "        data = cursor.fetchall()\n",
    "        # if else untuk pengecekan input data\n",
    "        if data:\n",
    "            print(\"Data exists\")\n",
    "        else:\n",
    "            #   scrape detail sesuai link\n",
    "            r2 = Request(link, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'})\n",
    "            print(r2)\n",
    "            response2 = urlopen(r2).read()\n",
    "            print(type(response2))\n",
    "            \n",
    "            soup2 = BeautifulSoup(response2, \"html.parser\")\n",
    "            print(type(soup2))\n",
    "            raw_deskripsi = soup2.find('div', \"tdd-opportunities-section__new\")\n",
    "            # increment deskripsi\n",
    "            deskripsi = \"\"\n",
    "            for string in raw_deskripsi.strings:\n",
    "                new_string  = deskripsi + \" \" + string\n",
    "                deskripsi = new_string\n",
    "\n",
    "            # menghilangkan Apostrophe mark\n",
    "            # deskripsi.replace(\"'\",\"\")\n",
    "\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO karir(title_lowongan, nama_perusahaan, lokasi_perusahaan, deskripsi_lowongan, link_lowongan)\"\n",
    "                \"VALUES ('\"+ lowongan.replace(\"'\", \"\").replace('\"', '') +\"', '\"+ perusahaan.replace(\"'\", \"\").replace('\"', '') +\"', '\"+ lokasi.replace(\"'\", \"\").replace('\"', '') +\"', '\"+ deskripsi.replace(\"'\", \"\").replace('\"', '') +\"', '\"+ link +\"')\"\n",
    "                )\n",
    "            print(\"Data has been added\")\n",
    "    \n",
    "#     title_lowongan.append(lowongan)\n",
    "#     nama_perusahaan.append(perusahaan)\n",
    "#     lokasi_perusahaan.append(lokasi)\n",
    "#     deskripsi_lowongan.append(deskripsi)\n",
    "#     link_lowongan.append(link)\n",
    "\n",
    "# jobList_dict ={'lowongan':title_lowongan, 'perusahaan':nama_perusahaan, 'lokasi':lokasi_perusahaan, 'deskripsi':deskripsi_lowongan, 'link':link_lowongan}\n",
    "# df = pd.DataFrame(jobList_dict,columns = ['lowongan','perusahaan','lokasi', 'deskripsi', 'link'])\n",
    "\n",
    "db.commit()\n",
    "\n",
    "# df.sort_values('lowongan',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}